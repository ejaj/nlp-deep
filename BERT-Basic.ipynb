{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "64497f81-d864-4f2f-add9-8a61f401173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from tqdm import tqdm\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aa101c83-3c99-470d-a576-450b60edb458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/spamdata_v2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "542195eb-6a9c-4306-96bf-f13294661b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.865937\n",
       "1    0.134063\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cec8e5ad-ee6f-4f09-87ff-39fcb2fd44d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dataset into train, validation and test sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
    "                                                                    random_state=40, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=40, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1205bd5f-8c8b-4f42-bfe1-20fee393a2c5",
   "metadata": {},
   "source": [
    "# Import Bert - base- uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "80b0787c-c2f5-4597-b383-32bd77bd7d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f5489c9e-1f8c-4478-aca2-d4dae1f7ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = [len(i.split()) for i in train_text]\n",
    "# seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fd7fb342-0355-41f8-9f1a-72fcf4b11f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b90f70-e3dd-4bde-9edc-1ee706fceacd",
   "metadata": {},
   "source": [
    "# Tokenize & Encode the Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6200a494-9953-44a4-9483-2fe8bb55ca9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kazi/Works/Projects/nlp-deep/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length =  25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length=25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = 25,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b0e3c7f6-ac92-4206-b508-3c39726cb5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4a01d9b8-d51f-493d-81b2-e889f06977e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert lists to tensors\n",
    "\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a9cd7953-6f94-43e1-a22b-229857a47810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21feefd1-64f3-4e13-81e6-3d151f47f852",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "938d0fb1-24c3-4bfa-9ad9-bc4b7b765643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f72e5334-b9a7-47e5-b604-2655e9ef0e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        \n",
    "        self.bert = bert \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        \n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "      \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ff15d7bc-03b5-4832-82b9-1f3ae32a62ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "12c01b17-b039-4e63-b4ef-45487089ff8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kazi/Works/Projects/nlp-deep/venv/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9bb4f9cc-8cb4-47e3-8305-a3c9eaa3fcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [0.57743559 3.72848948]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# compute the class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "\n",
    "print(\"Class Weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "03ad389b-a805-40f8-9f3d-b3b7a7348737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1f21f8da-2a7f-40b9-bfdd-2785fb7876f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optimizer, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train the model on the training set.\n",
    "    \n",
    "    Parameters:\n",
    "    model -- PyTorch model to be trained\n",
    "    train_dataloader -- DataLoader for the training set\n",
    "    optimizer -- optimizer for updating model parameters\n",
    "    device -- device on which to perform training (CPU or GPU)\n",
    "    \n",
    "    Returns:\n",
    "    avg_loss -- average loss on the training set\n",
    "    total_preds -- model predictions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the device is available\n",
    "        if device == 'cuda' and not torch.cuda.is_available():\n",
    "            print(\"CUDA device not available. Falling back to CPU.\")\n",
    "            device = 'cpu'\n",
    "\n",
    "        # Move model to the device\n",
    "        model.to(device)\n",
    "\n",
    "        model.train()\n",
    "        total_loss, total_accuracy = 0, 0\n",
    "\n",
    "        # Empty list to save model predictions\n",
    "        total_preds = []\n",
    "\n",
    "        # Iterate over batches\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Training\")):\n",
    "            # Push the batch to the device\n",
    "            batch = [r.to(device) for r in batch]\n",
    "\n",
    "            sent_id, mask, labels = batch\n",
    "\n",
    "            # Clear previously calculated gradients \n",
    "            model.zero_grad()\n",
    "\n",
    "            # Get model predictions for the current batch\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # Compute the loss between actual and predicted values\n",
    "            loss = cross_entropy(preds, labels)\n",
    "\n",
    "            # Add on to the total loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backward pass to calculate the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the gradients to 1.0 to prevent the exploding gradient problem\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Model predictions are stored on GPU. So, push them to CPU\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            # Append the model predictions\n",
    "            total_preds.append(preds)\n",
    "\n",
    "        # Compute the training loss of the epoch\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        # Reshape the predictions in form of (number of samples, no. of classes)\n",
    "        total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "        # Return the loss and predictions\n",
    "        return avg_loss, total_preds\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if 'CUDA error' in str(e):\n",
    "            print(\"CUDA error occurred. Falling back to CPU.\")\n",
    "            return train(model, train_dataloader, optimizer, device='cpu')\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "58bd5ff2-f9fe-49ec-8aed-59ca6cddf0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set.\n",
    "    \n",
    "    Parameters:\n",
    "    model -- PyTorch model to be evaluated\n",
    "    val_dataloader -- DataLoader for the validation set\n",
    "    device -- device on which to perform evaluation (CPU or GPU)\n",
    "    \n",
    "    Returns:\n",
    "    avg_loss -- average loss on the validation set\n",
    "    total_preds -- model predictions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the device is available\n",
    "        if device == 'cuda' and not torch.cuda.is_available():\n",
    "            print(\"CUDA device not available. Falling back to CPU.\")\n",
    "            device = 'cpu'\n",
    "\n",
    "        # Move model to the device\n",
    "        model.to(device)\n",
    "\n",
    "        print(\"\\nEvaluating...\")\n",
    "\n",
    "        # Deactivate dropout layers\n",
    "        model.eval()\n",
    "\n",
    "        total_loss, total_accuracy = 0, 0\n",
    "\n",
    "        # Empty list to save the model predictions\n",
    "        total_preds = []\n",
    "\n",
    "        # Iterate over batches\n",
    "        for step, batch in enumerate(tqdm(val_dataloader, desc=\"Evaluating\")):\n",
    "            # Push the batch to the device\n",
    "            batch = [t.to(device) for t in batch]\n",
    "\n",
    "            sent_id, mask, labels = batch\n",
    "\n",
    "            # Deactivate autograd\n",
    "            with torch.no_grad():\n",
    "                # Model predictions\n",
    "                preds = model(sent_id, mask)\n",
    "\n",
    "                # Compute the validation loss between actual and predicted values\n",
    "                loss = cross_entropy(preds, labels)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                preds = preds.detach().cpu().numpy()\n",
    "\n",
    "                total_preds.append(preds)\n",
    "\n",
    "        # Compute the validation loss of the epoch\n",
    "        avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "        # Reshape the predictions in form of (number of samples, no. of classes)\n",
    "        total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "        return avg_loss, total_preds\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if 'CUDA error' in str(e):\n",
    "            print(\"CUDA error occurred. Falling back to CPU.\")\n",
    "            return evaluate(model, val_dataloader, device='cpu')\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c9dcf6b1-f195-4867-a785-f2d19c84f6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                             | 0/122 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA error occurred. Falling back to CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                             | 0/122 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper_nll_loss_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 41\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Get model predictions for the current batch\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Compute the loss between actual and predicted values\u001b[39;00m\n",
      "File \u001b[0;32m~/Works/Projects/nlp-deep/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[84], line 27\u001b[0m, in \u001b[0;36mBERT_Arch.forward\u001b[0;34m(self, sent_id, mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sent_id, mask):\n\u001b[1;32m     25\u001b[0m     \n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#pass the inputs to the model  \u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     _, cls_hs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(cls_hs)\n",
      "File \u001b[0;32m~/Works/Projects/nlp-deep/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[0;32m~/Works/Projects/nlp-deep/venv/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:990\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[0;32m--> 990\u001b[0m extended_attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_extended_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# If a 2D or 3D attention mask is provided for the cross-attention\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\u001b[39;00m\n",
      "File \u001b[0;32m~/Works/Projects/nlp-deep/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:726\u001b[0m, in \u001b[0;36mModuleUtilsMixin.get_extended_attention_mask\u001b[0;34m(self, attention_mask, input_shape, device)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;66;03m# Since attention_mask is 1.0 for positions we want to attend and 0.0 for\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;66;03m# masked positions, this operation will create a tensor which is 0.0 for\u001b[39;00m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;66;03m# positions we want to attend and -10000.0 for masked positions.\u001b[39;00m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# Since we are adding it to the raw scores before the softmax, this is\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# effectively the same as removing these entirely.\u001b[39;00m\n\u001b[0;32m--> 726\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43mextended_attention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# fp16 compatibility\u001b[39;00m\n\u001b[1;32m    727\u001b[0m extended_attention_mask \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m extended_attention_mask) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10000.0\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Epoch \u001b[39m\u001b[38;5;132;01m{:}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{:}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, epochs))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#train model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m train_loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#evaluate model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m valid_loss, _ \u001b[38;5;241m=\u001b[39m evaluate()\n",
      "Cell \u001b[0;32mIn[89], line 76\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA error\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA error occurred. Falling back to CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[89], line 78\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train(model, train_dataloader, optimizer, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[89], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, optimizer, device)\u001b[0m\n\u001b[1;32m     41\u001b[0m preds \u001b[38;5;241m=\u001b[39m model(sent_id, mask)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Compute the loss between actual and predicted values\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Add on to the total loss\u001b[39;00m\n\u001b[1;32m     47\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Works/Projects/nlp-deep/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Works/Projects/nlp-deep/venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:211\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Works/Projects/nlp-deep/venv/lib/python3.10/site-packages/torch/nn/functional.py:2689\u001b[0m, in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2688\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper_nll_loss_forward)"
     ]
    }
   ],
   "source": [
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train(model, train_dataloader, optimizer, device='cuda')\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    valid_loss, total_preds = evaluate(model, val_dataloader, device='cuda')\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4eb76b-701d-4232-8151-66bbb6f9c971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa9e229-a161-430e-b143-702cba125402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
