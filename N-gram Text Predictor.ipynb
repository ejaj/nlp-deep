{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3fb5d-d688-4529-b444-f4eec256d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f245ae6-667b-4867-8956-ca0ec271235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "file_path = data_dir + \"/en_US.twitter.txt\"\n",
    "\n",
    "## nltk settings\n",
    "nltk.data.path.append(data_dir)\n",
    "nltk.download('punkt')\n",
    "\n",
    "## Opening the File in read mode (\"r\")\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f3fc25-34be-49c7-ad29-63a678517d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(data):\n",
    "    # Split by newline character, strip spaces, drop empty lines, convert to lowercase, and tokenize\n",
    "    return [nltk.word_tokenize(sentence.lower().strip()) for sentence in data.split('\\n') if sentence.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba5ee4-c387-493c-bbb1-49b37682f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pass our data to this function    \n",
    "tokenized_sentences = preprocess_pipeline(data)\n",
    "print(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4536c-64f4-4b48-87b0-e38a078b3e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtain Train and Test Split \n",
    "train, test = train_test_split(tokenized_sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "## Obtain Train and Validation Split \n",
    "train, val = train_test_split(train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d34b36-d14b-4328-b31c-bce972847482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Dictionary\n",
    "def count_the_words(sentences) -> 'dict':\n",
    "    # Creating a Dictionary of counts\n",
    "    word_counts = {}\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            if token not in word_counts.keys():\n",
    "                word_counts[token] = 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa94d77-ae61-4497-86c8-559830ebbd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Out-of-vocabulary words\n",
    "def handling_oov(tokenized_sentences, count_threshold) -> 'list':\n",
    "    # Empty list for closed vocabulary\n",
    "    closed_vocabulary = []\n",
    "    word_cound = count_the_words(tokenized_sentences)\n",
    "    for word, count in words_count.items():\n",
    "        if count >= count_threshold :\n",
    "            closed_vocabulary.append(word)\n",
    "    return closed_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb09a0-156e-4ccb-be9a-c4a9582a7408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59657160-b005-46e8-abf6-76c545b9400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_tokenize(tokenized_sentences, vocabulary, unknown_token = \"<unk>\") -> 'list':\n",
    "\n",
    "  # Convert Vocabulary into a set\n",
    "  vocabulary = set(vocabulary)\n",
    "\n",
    "  # Create empty list for sentences\n",
    "  new_tokenized_sentences = []\n",
    "  \n",
    "  # Iterate over sentences\n",
    "  for sentence in tokenized_sentences:\n",
    "\n",
    "    # Iterate over sentence and add <unk> \n",
    "    # if the token is absent from the vocabulary\n",
    "    new_sentence = []\n",
    "    for token in sentence:\n",
    "      if token in vocabulary:\n",
    "        new_sentence.append(token)\n",
    "      else:\n",
    "        new_sentence.append(unknown_token)\n",
    "    \n",
    "    # Append sentece to the new list\n",
    "    new_tokenized_sentences.append(new_sentence)\n",
    "\n",
    "  return new_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4459d-1cd0-4acd-aa99-3c5e9e33a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansing(train_data, test_data, count_threshold):\n",
    "    \n",
    "  # Get closed Vocabulary\n",
    "  vocabulary = handling_oov(train_data, count_threshold)\n",
    "    \n",
    "  # Updated Training Dataset\n",
    "  new_train_data = unk_tokenize(train_data, vocabulary)\n",
    "    \n",
    "  # Updated Test Dataset\n",
    "  new_test_data = unk_tokenize(test_data, vocabulary)\n",
    "\n",
    "  return new_train_data, new_test_data, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6b316f-d107-4bef-b69b-81d34d0ff480",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 6\n",
    "final_train, final_test, vocabulary = cleansing(train, test, min_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa925dc2-5be8-43b5-9a80-76487357f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token = \"<s>\", end_token = \"<e>\") -> 'dict':\n",
    "\n",
    "  # Empty dict for n-grams\n",
    "  n_grams = {}\n",
    " \n",
    "  # Iterate over all sentences in the dataset\n",
    "  for sentence in data:\n",
    "        \n",
    "    # Append n start tokens and a single end token to the sentence\n",
    "    sentence = [start_token]*n + sentence + [end_token]\n",
    "    \n",
    "    # Convert the sentence into a tuple\n",
    "    sentence = tuple(sentence)\n",
    "\n",
    "    # Temp var to store length from start of n-gram to end\n",
    "    m = len(sentence) if n==1 else len(sentence)-1\n",
    "    \n",
    "    # Iterate over this length\n",
    "    for i in range(m):\n",
    "        \n",
    "      # Get the n-gram\n",
    "      n_gram = sentence[i:i+n]\n",
    "    \n",
    "      # Add the count of n-gram as value to our dictionary\n",
    "      # IF n-gram is already present\n",
    "      if n_gram in n_grams.keys():\n",
    "        n_grams[n_gram] += 1\n",
    "      # Add n-gram count\n",
    "      else:\n",
    "        n_grams[n_gram] = 1\n",
    "        \n",
    "  return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8293214-2f9d-4b66-ae21-6a4ee43eff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_for_single_word(word, previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary_size, k = 1.0) -> 'float':\n",
    "\n",
    "  # Convert the previous_n_gram into a tuple \n",
    "  previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "  # Calculating the count, if exists from our freq dictionary otherwise zero\n",
    "  previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n",
    "  \n",
    "  # The Denominator\n",
    "  denom = previous_n_gram_count + k * vocabulary_size\n",
    "\n",
    "  # previous n-gram plus the current word as a tuple\n",
    "  nplus1_gram = previous_n_gram + (word,)\n",
    "\n",
    "  # Calculating the nplus1 count, if exists from our freq dictionary otherwise zero \n",
    "  nplus1_gram_count = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n",
    "\n",
    "  # Numerator\n",
    "  num = nplus1_gram_count + k\n",
    "\n",
    "  # Final Fraction\n",
    "  prob = num / denom\n",
    "  return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5154a55-14bb-47c1-8c93-34c4fec3942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs(previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0) -> 'dict':\n",
    "\n",
    "  # Convert to Tuple\n",
    "  previous_n_gram = tuple(previous_n_gram)\n",
    "\n",
    "  # Add end and unknown tokens to the vocabulary\n",
    "  vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
    "\n",
    "  # Calculate the size of the vocabulary\n",
    "  vocabulary_size = len(vocabulary)\n",
    "\n",
    "  # Empty dict for probabilites\n",
    "  probabilities = {}\n",
    "\n",
    "  # Iterate over words \n",
    "  for word in vocabulary:\n",
    "    \n",
    "    # Calculate probability\n",
    "    probability = prob_for_single_word(word, previous_n_gram, \n",
    "                                           n_gram_counts, nplus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)\n",
    "    # Create mapping: word -> probability\n",
    "    probabilities[word] = probability\n",
    "\n",
    "  return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828e30e-cdd7-4bdc-bff7-8cde9f58a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_complete(previous_tokens, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0, start_with=None):\n",
    "\n",
    "    \n",
    "    # length of previous words\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    \n",
    "    # most recent 'n' words\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    \n",
    "    # Calculate probabilty for all words\n",
    "    probabilities = probs(previous_n_gram,n_gram_counts, nplus1_gram_counts,vocabulary, k=k)\n",
    "\n",
    "    # Intialize the suggestion and max probability\n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "\n",
    "    # Iterate over all words and probabilites, returning the max.\n",
    "    # We also add a check if the start_with parameter is provided\n",
    "    for word, prob in probabilities.items():\n",
    "        \n",
    "        if start_with != None: \n",
    "            \n",
    "            if not word.startswith(start_with):\n",
    "                continue \n",
    "\n",
    "        if prob > max_prob: \n",
    "\n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "\n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dea7df-d7f0-4db5-81e6-19fb168473ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "\n",
    "    # See how many models we have\n",
    "    count = len(n_gram_counts_list)\n",
    "    \n",
    "    # Empty list for suggestions\n",
    "    suggestions = []\n",
    "    \n",
    "    # IMP: Earlier \"-1\"\n",
    "    \n",
    "    # Loop over counts\n",
    "    for i in range(count-1):\n",
    "        \n",
    "        # get n and nplus1 counts\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        nplus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        \n",
    "        # get suggestions \n",
    "        suggestion = auto_complete(previous_tokens, n_gram_counts,\n",
    "                                    nplus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "        # Append to list\n",
    "        suggestions.append(suggestion)\n",
    "        \n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d769453-cb83-4769-b382-3b3cd9e85f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_counts_list = []\n",
    "for n in range(1, 6):\n",
    "    n_model_counts = count_n_grams(final_train, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17139ab-9146-4e9e-9bae-6e2743339492",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_tokens = [\"i\", \"was\", \"about\"]\n",
    "suggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "display(suggestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb0ec0-d30e-408e-94c9-60a05e39ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"unigram count:\" , len(n_gram_counts_list[0]))\n",
    "print(\"bigram count:\", len(n_gram_counts_list[1]))\n",
    "print(\"trigram count:\", len(n_gram_counts_list[2]))\n",
    "print(\"quadgram count:\", len(n_gram_counts_list[3]))\n",
    "print(\"quintgram count:\", len(n_gram_counts_list[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e67718-d709-4f8b-84e7-24f890e400c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing to file\n",
    "with open(\"en_counts.txt\", 'wb') as f:\n",
    "    pickle.dump(n_gram_counts_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2470938a-e342-4678-a074-0c483febbf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing to file\n",
    "with open(\"vocab.txt\", 'wb') as f:\n",
    "    pickle.dump(vocabulary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a96a14-5383-405d-b83e-4c7050da275e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
